{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019년 12월 6일 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext\n",
    "%reload_ext\n",
    "%unload_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext # 충돌이 일어나면 reload_Ext 써서 다시 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cho 2019-12-09 \n",
      "\n",
      "tensorflow 2.0.0\n",
      "tensorboard 2.0.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -acho -d -ptensorflow,tensorboard\n",
    "# 나중에 버전 확인 용도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: version-information in c:\\users\\cho gyung ah\\anaconda3\\lib\\site-packages (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "# watermark보다 이쁜애\n",
    "# 쓰는 패키지 보여주는 것 \n",
    "!pip install version-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.7.3 64bit [MSC v.1915 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "7.4.0"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.17134 SP0"
        },
        {
         "module": "numpy",
         "version": "1.17.4"
        },
        {
         "module": "scipy",
         "version": "1.3.3"
        },
        {
         "module": "pandas",
         "version": "0.24.2"
        },
        {
         "module": "tensorflow",
         "version": "2.0.0"
        },
        {
         "module": "keras",
         "version": "2.3.0"
        },
        {
         "module": "tensorboard",
         "version": "2.0.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.7.3 64bit [MSC v.1915 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>7.4.0</td></tr><tr><td>OS</td><td>Windows 10 10.0.17134 SP0</td></tr><tr><td>numpy</td><td>1.17.4</td></tr><tr><td>scipy</td><td>1.3.3</td></tr><tr><td>pandas</td><td>0.24.2</td></tr><tr><td>tensorflow</td><td>2.0.0</td></tr><tr><td>keras</td><td>2.3.0</td></tr><tr><td>tensorboard</td><td>2.0.0</td></tr><tr><td colspan='2'>Fri Dec 06 14:28:17 2019 ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.7.3 64bit [MSC v.1915 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 7.4.0 \\\\ \\hline\n",
       "OS & Windows 10 10.0.17134 SP0 \\\\ \\hline\n",
       "numpy & 1.17.4 \\\\ \\hline\n",
       "scipy & 1.3.3 \\\\ \\hline\n",
       "pandas & 0.24.2 \\\\ \\hline\n",
       "tensorflow & 2.0.0 \\\\ \\hline\n",
       "keras & 2.3.0 \\\\ \\hline\n",
       "tensorboard & 2.0.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Fri Dec 06 14:28:17 2019 ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.7.3 64bit [MSC v.1915 64 bit (AMD64)]\n",
       "IPython 7.4.0\n",
       "OS Windows 10 10.0.17134 SP0\n",
       "numpy 1.17.4\n",
       "scipy 1.3.3\n",
       "pandas 0.24.2\n",
       "tensorflow 2.0.0\n",
       "keras 2.3.0\n",
       "tensorboard 2.0.0\n",
       "Fri Dec 06 14:28:17 2019 ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%version_information numpy, scipy, pandas, tensorflow, keras, tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# 누적이 되므로 지워준다. \n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir mylogdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import datetime \n",
    "# time: 절대적 시각\n",
    "# caldendar: 달 \n",
    "# datetime: 시간 날짜 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n",
    "# 2.0 부터 tensorboard 쓸 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장될 경로\n",
    "# cho=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# 인스턴스\n",
    "# 저장경로 설정 안해놓으면 현재 작업하는 경로에 저장 \n",
    "# binary file -> tensorboard가 웹서브에 띄워준다. \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard('remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 21s 350us/sample - loss: 0.2209 - accuracy: 0.9350 - val_loss: 0.1008 - val_accuracy: 0.9694\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 21s 347us/sample - loss: 0.0969 - accuracy: 0.9707 - val_loss: 0.0801 - val_accuracy: 0.9762\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 20s 329us/sample - loss: 0.0698 - accuracy: 0.9779 - val_loss: 0.0693 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c7ba4a780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=3, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/Cho Gyung Ah/Documents/Github/image\n"
     ]
    }
   ],
   "source": [
    "# 현재 작업 폴더 \n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12424), started 0:01:27 ago. (Use '!kill 12424' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-134938c0fc0d2078\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-134938c0fc0d2078\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 보안이슈때문에 안나오는 경우가 있다. \n",
    "%tensorboard --logdir remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard_image보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension doesn't define how to unload it.\n"
     ]
    }
   ],
   "source": [
    "%unload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- objective 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data. The data is already divided into train and test.\n",
    "# The labels are integers representing classes.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = \\\n",
    "    fashion_mnist.load_data()\n",
    "\n",
    "# Names of the integer classes, i.e., 0 -> T-short/top, 1 -> Trouser, etc.\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (28, 28)\n",
      "Label:  9 -> Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape: \", train_images[0].shape)\n",
    "print(\"Label: \", train_labels[0], \"->\", class_names[train_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image for the Summary API.\n",
    "# CNN은 3차원 \n",
    "img = np.reshape(train_images[0], (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'logs/train': Directory not empty\n",
      "rm: cannot remove 'logs/validation': Directory not empty\n"
     ]
    }
   ],
   "source": [
    "# Clear out any prior log data.\n",
    "!rm -rf logs\n",
    "\n",
    "# Sets up a timestamped log directory.\n",
    "# logdir = \"logs/train_data/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Creates a file writer for the log directory.\n",
    "# create_file_write: 어디에 저장할건지 지정해준다. \n",
    "file_writer = tf.summary.create_file_writer('remove1')\n",
    "\n",
    "# Using the file writer, log the reshaped image.\n",
    "# 이건 그냥 암기해야한다. \n",
    "with file_writer.as_default():\n",
    "  tf.summary.image(\"Training data\", img, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SummaryWriter',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'audio',\n",
       " 'create_file_writer',\n",
       " 'create_noop_writer',\n",
       " 'experimental',\n",
       " 'flush',\n",
       " 'histogram',\n",
       " 'image',\n",
       " 'record_if',\n",
       " 'scalar',\n",
       " 'text',\n",
       " 'trace_export',\n",
       " 'trace_off',\n",
       " 'trace_on',\n",
       " 'write']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.summary)\n",
    "# SummaryWriter: summary 집어 넣는 애\n",
    "# create_file_writer: djeldpekrk wjwkdgownsms rjt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%writefile` not found (But cell magic `%%writefile` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "# 기존에 이름이 똑같은게 있으면 에러가 난다. \n",
    "# %writefile hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_recreate_cm',\n",
       " 'args',\n",
       " 'func',\n",
       " 'gen',\n",
       " 'kwds']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(xx.as_default())\n",
    "# __exit__가 나온다. -> with 구문을 쓰라는 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.aa가 저장된다. \n",
    "with open('a.aa','w') as f:\n",
    "    f.write('asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터가 덮어씌워진다. \n",
    "# 폴더를 만들어준다. \n",
    "xx = tf.summary.create_file_writer('hi',img)\n",
    "\n",
    "with xx.as_default():\n",
    "    # mainthred \n",
    "    tf.summary.image('adsf',img, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9072), started 0:00:55 ago. (Use '!kill 9072' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-51cf4084b5ad4dde\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-51cf4084b5ad4dde\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir remove1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAXYEAR',\n",
       " 'MINYEAR',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'date',\n",
       " 'datetime',\n",
       " 'datetime_CAPI',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'timezone',\n",
       " 'tzinfo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 날짜를 관리해준다. \n",
    "x = datetime.date(2019,12,29)\n",
    "y = datetime.date(2019,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = datetime.timedelta(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 12, 6, 16, 9, 18, 696077)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사람이 읽기 쉬운 것 \n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1575616167.0386617"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()\n",
    "# 1970년부터 ms씩 증가시킨 애"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29-12-2019'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strf 원하는 format으로 바꿔주는것 \n",
    "datetime.datetime.strftime(x, '%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 12, 13, 0, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strp 문자열로부터 형태를 객체로 만들어주는것\n",
    "datetime.datetime.strptime('2019-12-13', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간, 날짜는 python 포맷 \n",
    "# dateutil은 pandas에 기본으로 깔려있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logdir 붙이고 다시 실행  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out any prior log data.\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a timestamped log directory.\n",
    "# 날짜시간이 너무 길어서 이름 치는게 너무 귀찮을 수 있음...\n",
    "logdir = \"logs/train_data/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Creates a file writer for the log directory.\n",
    "# 파일이 비어있는 directory이면 에러가 나지 않는다. \n",
    "# tf.summary는 데이터 확인하는 용도로 다양한기능을 제공하고 있다.\n",
    "file_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# Using the file writer, log the reshaped image.\n",
    "# 최근파일에 내용이 as_default()를 이용하여 덮어진다. \n",
    "with file_writer.as_default():\n",
    "  tf.summary.image(\"Training data\", img, step=0, max_outputs=4)\n",
    "# step은 이미지 부제목? 같은거 위에 뜬다. \n",
    "# step: (int64를 monotonic level 으로 바꿔준다.): 대조레벨 (흑백이라서 잘 안보임)\n",
    "# max_outputs 를 4로하면 한꺼번에 사진을 4개 볼 수 있도록 도와준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1324), started 0:02:09 ago. (Use '!kill 1324' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cbb3d112d9a37f8f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cbb3d112d9a37f8f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch에도 tensorboard가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %으로 안열고 !로 열면 load_ext 안해도 새로운 페이지에서 뜬다.\n",
    "# 다만 실행시키는동안 걔속 돌아가서, 중지시켜야한다. \n",
    "!tensorboard --logdir logs/train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 불러들일 때 \n",
    "1. imageDataGenerator: 속도가 느려서 tensor로 불러들인다. \n",
    "image를 관리하고 싶을 때 이미지를 tensorflow는 이미지를 기본적으로 bytes형태로 쓴다. \n",
    "\n",
    "python은 open을 (파일 스트리밍 객체) 메모리 파일 비슷한걸 만들어써서 덮어씀. \n",
    "\n",
    "2. 쌩으로 tensor로 (numpy로) 불러오는 것 \n",
    "\n",
    "\n",
    "\n",
    "bytesio, stringio같은 것들이 있다. 효과적으로 관리할 수 있음. \n",
    "\n",
    "GAN은 generative 모델 \n",
    "\n",
    "matplotlib은 savefig같은게 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킬때와 학습시킬때 이미지 차원이 똑같아야한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras: callback\n",
    "\n",
    "tensorflow: tf.summary() 데이터 확인하는 용도로 다양한기능을 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019년 12월 9일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Nets for Vision \n",
    "\n",
    "\n",
    "- End-To-End: 병목현상이 없어서 좋다. \n",
    "\n",
    "IDEA # 1\n",
    "Learn features from data\n",
    "IDEA # 2\n",
    "Use differentiable functions that produce\n",
    "features efficiently\n",
    "IDEA # 3\n",
    "End-to-end learning:\n",
    "no distinction between feature extractor and classifier\n",
    "IDEA # 4\n",
    "“Deep” architectures:\n",
    "cascade of simpler non-linear modules\n",
    "\n",
    "- Back propagation : Chin Rule (쪼개는 것) \n",
    "\n",
    "\n",
    "### feature selection \n",
    "\n",
    "- filter: 통계 방법-feature importance\n",
    "- wrapper: 기계학습과 동시해해서 back word\n",
    "- embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected Neural Net\n",
    "이미지는 특정 부분 주위에 있는 것과 큰 연관성이 있다. \n",
    "\n",
    "### Locally Connected Neural Net\n",
    "부분적으로 몇개 특징만 보는 것. 이렇게 하면 parameter 개수가 줄어든다. 'Ocam 의 면도날': 성능이 비슷하니깐 간단한걸 쓴다. \n",
    "\n",
    "### Convolutional Net\n",
    "하나의 영역 안에 있는 특징을 찾는다.  \n",
    "- pooling: 모델이 간단해져 속도가 빨라진다. (요즘엔 스트라이드를 더 쓴다.) \n",
    "- Stride: 몇 칸씩 옮기는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import convolve, correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신호처리하는데서 많이 썼었다. \n",
    "# np.은 2차원을 계산하지 못한다. \n",
    "np.convolve([3],[1,2])\n",
    "# 3*1, 3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve([3,2],[1,2])\n",
    "# 3*1 ,3*2+2*1, 2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.correlate([3],[1,2])\n",
    "# 3*2, 3*1\n",
    "# 교차 상관 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10, 13, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve([1,2],[3,4,5])\n",
    "# 3*1,1*4+2*3, 2*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 11])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.correlate([1,2],[3,4,5])\n",
    "# 5*2 +1*4, 2*4,1*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve, convolve2d\n",
    "# convolve 는 2차원도 할 수 있다. \n",
    "# 따라서 이걸 더 많이 쓴다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  4],\n",
       "       [ 3, 10,  8]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve([[1,2],[3,4]],[[1,2]])\n",
    "# 1*1, 1*2+2*1, 2*2\n",
    "# 3*1, 3*6+4*1, 4*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 10,  8],\n",
       "       [10, 28, 20],\n",
       "       [ 3, 10,  8]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve2d([[1,2],[3,4]], [[3,4],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/conv_filter_visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# tensorflow 불러오기 전에 실행시키면 v1 코드를 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_math_ops.sqrt(x, name=None)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-07"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version2가 최근에 나왔기 때문에 v1,2를 나눠서 불러들일 수 있다. \n",
    "# tf.compat.v1.keras.backend.epsilon\n",
    "# tf.compat.v2.keras.backend.epsilon 두개의 버전이 다르다. \n",
    "tf.compat.v1.keras.backend.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-07"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v2.keras.backend.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "# zero division을 방지하기 위해서 epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]- 넘파이에서 짤라 내는 것 \n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "# tensorflow는 28,28,1 로 들어가는데 ,티아노는 1, 28, 28로 들어간다. \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "#     0 보다 작으면0 , 255보다 크면 255 \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  0,  0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(a>3, 0, -1)\n",
    "# 조건에 맞으면 0, 안맞으면 -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 4, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(a,2,4)\n",
    "# 2보다 작으면 2, 4보다 크면 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector 값을 이미지로 바꿀 때 0-255 사이로 두어야함.\n",
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(412, 412),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\"\n",
    "    def _generate_filter_image(input_img,\n",
    "                               layer_output,\n",
    "                               filter_index):\n",
    "        \"\"\"Generates image for one particular filter.\n",
    "\n",
    "        # Arguments\n",
    "            input_img: The input-image Tensor.\n",
    "            layer_output: The output-image Tensor.\n",
    "            filter_index: The to be processed filter number.\n",
    "                          Assumed to be valid.\n",
    "\n",
    "        #Returns\n",
    "            Either None if no image could be generated.\n",
    "            or a tuple of the image (array) itself and the last loss.\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        # loss를 input_img 관해서 미분 \n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random(\n",
    "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "        else:\n",
    "            input_img_data = np.random.random(\n",
    "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # Slowly upscaling towards the original size prevents\n",
    "        # a dominating high-frequency of the to visualized structure\n",
    "        # as it would occur if we directly compute the 412d-image.\n",
    "        # Behaves as a better starting point for each following dimension\n",
    "        # and therefore avoids poor local minima\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # we run gradient ascent for e.g. 20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value * step\n",
    "\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                if loss_value <= K.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # Calculate upscaled dimension\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # Upscale\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = np.expand_dims(\n",
    "                process_image(img, input_img_data[0]), 0)\n",
    "\n",
    "        # decode the resulting input image\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  loss_value,\n",
    "                                                                  e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"Draw the best filters in a nxn grid.\n",
    "\n",
    "        # Arguments\n",
    "            filters: A List of generated images and their corresponding losses\n",
    "                     for each processed filter.\n",
    "            n: dimension of the grid.\n",
    "               If none, the largest possible square will be used\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # the filters that have the highest loss are assumed to be better-looking.\n",
    "        # we will only keep the top n*n filters.\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # build a black picture with enough space for\n",
    "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # fill the picture with our saved filters\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "    # this is the placeholder for the input images\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "    output_layer = layer_dict[layer_name]\n",
    "    assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "    # Compute to be processed filter range\n",
    "    filter_lower = filter_range[0]\n",
    "    filter_upper = (filter_range[1]\n",
    "                    if filter_range[1] is not None\n",
    "                    else len(output_layer.get_weights()[1]))\n",
    "    assert(filter_lower >= 0\n",
    "           and filter_upper <= len(output_layer.get_weights()[1])\n",
    "           and filter_upper > filter_lower)\n",
    "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "    # iterate through each filter and generate its corresponding image\n",
    "    processed_filters = []\n",
    "    for f in range(filter_lower, filter_upper):\n",
    "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "        if img_loss is not None:\n",
    "            processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # Finally draw and store the best filters to disk\n",
    "    _draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:47:46.354488 14592 deprecation.py:506] From C:\\Users\\Cho Gyung Ah\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.layers)\n",
    "# model.get_layer\n",
    "# __iter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x29205fb1c88>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x29205fb1668>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index/slice: sequence \n",
    "model.layers[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Network.get_layer of <tensorflow.python.keras.engine.sequential.Sequential object at 0x0000029205FB1208>>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent \n",
    "x = tf.ones((2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.gradient with 랑 같이 쓰면 eager tensor 방시으로 쓰고 with 안쓰면 옛날 방식으로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# __name__: 현재 파일이 import 안될 때 쓴다. \n",
    "# __name__: 현재 작업하고 있는 파일에서 name를 치면 __main이 나온다.\n",
    "# if __name__ == '__main__':\n",
    "#     # the name of the layer we want to visualize\n",
    "#     # (see model definition at keras/applications/vgg16.py)\n",
    "#     LAYER_NAME = 'block5_conv1'\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "# included_top: fully connected 붙일지 안붙일지\n",
    "vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute filters 0 to 64\n",
      "Costs of filter   0:   164 ( 54.60s )\n",
      "Costs of filter   1:    63 ( 51.44s )\n",
      "Costs of filter   2:   365 ( 51.31s )\n",
      "Costs of filter   3:   325 ( 48.68s )\n",
      "Costs of filter   4:   251 ( 49.02s )\n",
      "Costs of filter   5:   108 ( 49.21s )\n",
      "Costs of filter   6:    77 ( 51.77s )\n",
      "Costs of filter   7:   222 ( 48.24s )\n",
      "Costs of filter   8:   130 ( 53.70s )\n",
      "Costs of filter   9:    71 ( 60.59s )\n",
      "Costs of filter  10:   171 ( 51.54s )\n",
      "Costs of filter  11:   110 ( 55.52s )\n",
      "Costs of filter  12:    91 ( 56.15s )\n",
      "Costs of filter  13:   118 ( 52.12s )\n",
      "Costs of filter  14:    51 ( 51.87s )\n",
      "Costs of filter  15:   328 ( 51.00s )\n",
      "Costs of filter  16:   136 ( 61.09s )\n",
      "Costs of filter  17:   123 ( 51.78s )\n",
      "Costs of filter  18:   620 ( 50.45s )\n",
      "Costs of filter  19:   139 ( 54.78s )\n",
      "Costs of filter  20:   180 ( 53.27s )\n",
      "Costs of filter  21:    75 ( 52.03s )\n",
      "Costs of filter  22:    95 ( 52.66s )\n",
      "Costs of filter  23:    84 ( 55.98s )\n",
      "Costs of filter  24:   343 ( 51.12s )\n",
      "Costs of filter  25:   115 ( 58.87s )\n",
      "Costs of filter  26:   129 ( 54.41s )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-dcc17f805aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example function call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvisualize_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'block1_conv2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-281502a90de7>\u001b[0m in \u001b[0;36mvisualize_layer\u001b[1;34m(model, layer_name, step, epochs, upscaling_steps, upscaling_factor, output_dim, filter_range)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mprocessed_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_lower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_upper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mimg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_generate_filter_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-281502a90de7>\u001b[0m in \u001b[0;36m_generate_filter_image\u001b[1;34m(input_img, layer_output, filter_index)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# we run gradient ascent for e.g. 20 steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_img_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[0minput_img_data\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrads_value\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3580\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3581\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3582\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example function call\n",
    "visualize_layer(vgg, 'block1_conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras version2로 업데이트 안된게 많다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gan\n",
    "\n",
    "Transfer learning을 짤 쓰면 그 모델을 가져와서 더 학습시킬 수 있다. (imagenet)\n",
    "\n",
    "- pythorch -> tensor (model comfort) \n",
    "- keras에서 가져오기 (VGG같은 모델: 간단하고 성능이 좋아서, transfer learning 하기가 좋다.) \n",
    "- Tensorhub (모델 모아둔 곳: 내가 필요한 모델 불러올 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer 모델 쓰는 법\n",
    "https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n03271574', 'electric_fan', 0.5685956), ('n04542943', 'waffle_iron', 0.06175959), ('n04398044', 'teapot', 0.05639047)]\n"
     ]
    }
   ],
   "source": [
    "# Classify ImageNet classes with ResNet50\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# layers 50\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'bono.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + 4 + 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
