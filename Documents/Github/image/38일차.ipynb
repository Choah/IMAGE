{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019년 12월 23일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators는 전통적인 기계학습을 할 수 있다.\n",
    "\n",
    "- 1.대 호환\n",
    "- 기계학습 지원\n",
    "- sklearn은 gpu 지원을 안한다. \n",
    "\n",
    "https://www.tensorflow.org/tutorials/estimator/premade\n",
    "\n",
    "tensorflow 2.0 feature map이라고...\n",
    "dataframe은 image data generator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\n",
      "8192/2194 [================================================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\n",
      "8192/573 [============================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "train_path = tf.keras.utils.get_file(\n",
    "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\n",
    "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "\n",
    "# The label column has now been removed from the features.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BaselineClassifier',\n",
       " 'BaselineEstimator',\n",
       " 'BaselineRegressor',\n",
       " 'BestExporter',\n",
       " 'BoostedTreesClassifier',\n",
       " 'BoostedTreesEstimator',\n",
       " 'BoostedTreesRegressor',\n",
       " 'CheckpointSaverHook',\n",
       " 'CheckpointSaverListener',\n",
       " 'DNNClassifier',\n",
       " 'DNNEstimator',\n",
       " 'DNNLinearCombinedClassifier',\n",
       " 'DNNLinearCombinedEstimator',\n",
       " 'DNNLinearCombinedRegressor',\n",
       " 'DNNRegressor',\n",
       " 'Estimator',\n",
       " 'EstimatorSpec',\n",
       " 'EvalSpec',\n",
       " 'Exporter',\n",
       " 'FeedFnHook',\n",
       " 'FinalExporter',\n",
       " 'FinalOpsHook',\n",
       " 'GlobalStepWaiterHook',\n",
       " 'LatestExporter',\n",
       " 'LinearClassifier',\n",
       " 'LinearEstimator',\n",
       " 'LinearRegressor',\n",
       " 'LoggingTensorHook',\n",
       " 'ModeKeys',\n",
       " 'NanLossDuringTrainingError',\n",
       " 'NanTensorHook',\n",
       " 'ProfilerHook',\n",
       " 'RunConfig',\n",
       " 'SecondOrStepTimer',\n",
       " 'SessionRunArgs',\n",
       " 'SessionRunContext',\n",
       " 'SessionRunHook',\n",
       " 'SessionRunValues',\n",
       " 'StepCounterHook',\n",
       " 'StopAtStepHook',\n",
       " 'SummarySaverHook',\n",
       " 'TrainSpec',\n",
       " 'VocabInfo',\n",
       " 'WarmStartSettings',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'add_metrics',\n",
       " 'classifier_parse_example_spec',\n",
       " 'experimental',\n",
       " 'export',\n",
       " 'regressor_parse_example_spec',\n",
       " 'train_and_evaluate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 (DNN)\n",
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 바꿔주는 것 \n",
    "# from_tensor_slices가 좀 더 속도가 빠르다. \n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "# shuffle, repeat, batch, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "\n",
    "tensorflow가 sklearn의 영역에도 지원하려고 한다. \n",
    "정형 데이터 또한 판다스를 지원하고 있다. \n",
    "\n",
    "```python\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Temp\\tmp_uzf3hwh\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Temp\\\\tmp_uzf3hwh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FBAB3AFA90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# skit과 비슷하다. \n",
    "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 30 and 10 nodes respectively.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\head\\base_head.py:574: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adagrad.py:108: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\model_fn.py:337: scalar (from tensorflow.python.framework.tensor_shape) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.TensorShape([]).\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Temp\\tmp_uzf3hwh\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1413597, step = 0\n",
      "INFO:tensorflow:global_step/sec: 257.094\n",
      "INFO:tensorflow:loss = 0.9029021, step = 100 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.396\n",
      "INFO:tensorflow:loss = 0.8506154, step = 200 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.865\n",
      "INFO:tensorflow:loss = 0.8233807, step = 300 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.519\n",
      "INFO:tensorflow:loss = 0.7983245, step = 400 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.245\n",
      "INFO:tensorflow:loss = 0.76500857, step = 500 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.49\n",
      "INFO:tensorflow:loss = 0.73442197, step = 600 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.839\n",
      "INFO:tensorflow:loss = 0.7217271, step = 700 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.679\n",
      "INFO:tensorflow:loss = 0.69332194, step = 800 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.033\n",
      "INFO:tensorflow:loss = 0.6721415, step = 900 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 444.447\n",
      "INFO:tensorflow:loss = 0.64820755, step = 1000 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 396.06\n",
      "INFO:tensorflow:loss = 0.6482004, step = 1100 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.459\n",
      "INFO:tensorflow:loss = 0.6118206, step = 1200 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.767\n",
      "INFO:tensorflow:loss = 0.59617496, step = 1300 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.701\n",
      "INFO:tensorflow:loss = 0.5791475, step = 1400 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.381\n",
      "INFO:tensorflow:loss = 0.5709406, step = 1500 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.239\n",
      "INFO:tensorflow:loss = 0.5594651, step = 1600 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.179\n",
      "INFO:tensorflow:loss = 0.5489837, step = 1700 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.631\n",
      "INFO:tensorflow:loss = 0.5404053, step = 1800 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.362\n",
      "INFO:tensorflow:loss = 0.5182114, step = 1900 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.296\n",
      "INFO:tensorflow:loss = 0.51660854, step = 2000 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.341\n",
      "INFO:tensorflow:loss = 0.50517684, step = 2100 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.689\n",
      "INFO:tensorflow:loss = 0.49048904, step = 2200 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.364\n",
      "INFO:tensorflow:loss = 0.4900738, step = 2300 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.975\n",
      "INFO:tensorflow:loss = 0.4808503, step = 2400 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.141\n",
      "INFO:tensorflow:loss = 0.46442938, step = 2500 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.902\n",
      "INFO:tensorflow:loss = 0.46234334, step = 2600 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.619\n",
      "INFO:tensorflow:loss = 0.45480275, step = 2700 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.752\n",
      "INFO:tensorflow:loss = 0.46065956, step = 2800 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.763\n",
      "INFO:tensorflow:loss = 0.449146, step = 2900 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.89\n",
      "INFO:tensorflow:loss = 0.43662703, step = 3000 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.294\n",
      "INFO:tensorflow:loss = 0.4301403, step = 3100 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.092\n",
      "INFO:tensorflow:loss = 0.41747534, step = 3200 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.461\n",
      "INFO:tensorflow:loss = 0.42102933, step = 3300 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.461\n",
      "INFO:tensorflow:loss = 0.40619665, step = 3400 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.812\n",
      "INFO:tensorflow:loss = 0.3980481, step = 3500 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.512\n",
      "INFO:tensorflow:loss = 0.4074547, step = 3600 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.828\n",
      "INFO:tensorflow:loss = 0.3854711, step = 3700 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.803\n",
      "INFO:tensorflow:loss = 0.39826176, step = 3800 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.911\n",
      "INFO:tensorflow:loss = 0.38674653, step = 3900 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.387\n",
      "INFO:tensorflow:loss = 0.37916017, step = 4000 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 436.019\n",
      "INFO:tensorflow:loss = 0.36776704, step = 4100 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.737\n",
      "INFO:tensorflow:loss = 0.37128532, step = 4200 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.349\n",
      "INFO:tensorflow:loss = 0.37488103, step = 4300 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.447\n",
      "INFO:tensorflow:loss = 0.35555986, step = 4400 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.185\n",
      "INFO:tensorflow:loss = 0.35400856, step = 4500 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.061\n",
      "INFO:tensorflow:loss = 0.34017107, step = 4600 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.335\n",
      "INFO:tensorflow:loss = 0.34593806, step = 4700 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.781\n",
      "INFO:tensorflow:loss = 0.34158933, step = 4800 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 423.879\n",
      "INFO:tensorflow:loss = 0.33345944, step = 4900 (0.236 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Temp\\tmp_uzf3hwh\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.32910794.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x1fbab3afc88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "# estimator -> .train  (.fit)\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-23T14:32:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Temp\\tmp_uzf3hwh\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-23-14:32:35\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.93333334, average_loss = 0.39445177, global_step = 5000, loss = 0.39445177\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Temp\\tmp_uzf3hwh\\model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 0.933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 함수형 패러다임 \n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  케글에서 이긴 모델들 \n",
    "# stacking \n",
    "# xgboosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 설명가능한 neural network : what if .... (텐서플로우 학습 툴) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # 이미지를 [-1, 1]로 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor로 바꿔야지 속도가 더 빠르므로 이걸 쓴다.  -> tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 배치를 만들고 섞습니다.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    \n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # 주목: 배치사이즈로 None이 주어집니다.\n",
    "\n",
    "#   4x4 -> (3x3) -> (2x2) -> Conv2DTranspose -> (4x4)\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fbaf22dbe0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(64))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 메서드는 크로스 엔트로피 손실함수 (cross entropy loss)를 계산하기 위해 헬퍼 (helper) 함수를 반환합니다.\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00136353]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**핵심**\n",
    "\n",
    "피드백을 '생성자 손실함수'를 통해 받는다.\n",
    "\n",
    "생성자의 손실함수는 감별자를 얼마나 잘 속였는지에 대해 수치화를 합니다. 직관적으로 생성자가 원활히 수행되고 있다면, 감별자는 가짜 이미지를 진짜 (또는 1)로 분류를 할 것입니다. 여기서 우리는 생성된 이미지에 대한 감별자의 결정을 1로 이루어진 행렬과 비교를 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 잘 안되서 진행과정을 체크해봐야한다. \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "# epoch한번 돌때마다 저장하는 것: checkpoint \n",
    "# tf.keras.callback의 checkpoint를 썼는데, \n",
    "# tensorflow에도 checkpoint가 있었다. \n",
    "# ** 가변키워드 = 값으로 저장해야한다. \n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# 이 시드를 시간이 지나도 재활용하겠습니다. \n",
    "# (GIF 애니메이션에서 진전 내용을 시각화하는데 쉽기 때문입니다.) \n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tf.function`이 어떻게 사용되는지 주목해 주세요.\n",
    "# 이 데코레이터는 함수를 \"컴파일\"합니다.\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "#   처음나온 테크닉이라서 골치아픔 \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # GIF를 위한 이미지를 바로 생성합니다.\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # 15 에포크가 지날 때마다 모델을 저장합니다.\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # 마지막 에포크가 끝난 후 생성합니다.\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # `training`이 False로 맞춰진 것을 주목하세요.\n",
    "  # 이렇게 하면 (배치정규화를 포함하여) 모든 층들이 추론 모드로 실행됩니다. \n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 3 is 458.92352199554443 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\", line 1284, in time\n",
      "    out = eval(code, glob, local_ns)\n",
      "  File \"<timed eval>\", line 1, in <module>\n",
      "  File \"<ipython-input-50-bd83db03824b>\", line 6, in train\n",
      "    train_step(image_batch)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 487, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\n",
      "    ctx=ctx)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"D:\\01_Program\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크 숫자를 사용하여 하나의 이미지를 보여줍니다.\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gan\n",
    "\n",
    "- build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2DTranspose, BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(z_dim):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(256*7*7, input_dim = z_dim))\n",
    "    model.add(Reshape((7,7,256)))\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2 , padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 12544)             363776    \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DT (None, 14, 14, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 785       \n",
      "=================================================================\n",
      "Total params: 734,742\n",
      "Trainable params: 734,356\n",
      "Non-trainable params: 386\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_generator(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
